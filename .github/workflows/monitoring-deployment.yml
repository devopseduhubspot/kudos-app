# =========================================================
# Monitoring Stack Deployment Workflow
# =========================================================
# This workflow manages Prometheus and Grafana monitoring
# deployment on EKS clusters across different environments.
#
# Features:
# - Manual trigger with environment selection
# - Install/Uninstall monitoring stack
# - Environment-specific configurations
# - Comprehensive logging and status reporting
# =========================================================

name: monitoring-deployment

on:
  workflow_dispatch:
    inputs:
      environment:
        description: 'Target Environment'
        required: true
        default: 'dev'
        type: choice
        options:
          - dev
          - staging
          - prod
      action:
        description: 'Monitoring Action'
        required: true
        default: 'install'
        type: choice
        options:
          - install
          - uninstall
          - status
      helm_timeout:
        description: 'Helm Operation Timeout (minutes)'
        required: false
        default: '10'
        type: string

# ---------------------------------------------------------
# Environment Variables
# ---------------------------------------------------------
env:
  AWS_REGION: us-east-1
  EKS_CLUSTER_NAME: kudos-app-${{ github.event.inputs.environment || 'dev' }}
  MONITORING_NAMESPACE: monitoring
  HELM_RELEASE_NAME: kube-prometheus-stack
  HELM_TIMEOUT: ${{ github.event.inputs.helm_timeout || '10' }}m

# ---------------------------------------------------------
# Jobs
# ---------------------------------------------------------
jobs:
  # ---------------------------------------------------------
  # 1. Monitoring Stack Management
  # ---------------------------------------------------------
  manage-monitoring:
    name: "üìä ${{ github.event.inputs.action }} Monitoring Stack"
    runs-on: ubuntu-latest
    environment: ${{ github.event.inputs.environment || 'dev' }}
    
    steps:
      - name: "üîÅ Checkout Repository"
        uses: actions/checkout@v4

      - name: "üîë Configure AWS Credentials"
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: "‚öôÔ∏è Setup kubectl"
        uses: azure/setup-kubectl@v4
        with:
          version: v1.29.0

      - name: "üé° Setup Helm"
        uses: azure/setup-helm@v4
        with:
          version: v3.19.0

      - name: "üîó Update kubeconfig for EKS"
        run: |
          echo "üîç Environment: ${{ github.event.inputs.environment || 'dev' }}"
          echo "üéØ Target EKS cluster: $EKS_CLUSTER_NAME"
          echo "üåç AWS Region: $AWS_REGION"
          echo "üìä Action: ${{ github.event.inputs.action }}"
          
          # Test AWS CLI authentication
          echo "üîê Testing AWS CLI authentication..."
          aws sts get-caller-identity
          
          # Check if cluster exists
          echo "üîç Checking if EKS cluster exists..."
          if aws eks describe-cluster --name $EKS_CLUSTER_NAME --region $AWS_REGION; then
            echo "‚úÖ EKS cluster found: $EKS_CLUSTER_NAME"
          else
            echo "‚ùå EKS cluster not found or not accessible"
            echo "Available clusters:"
            aws eks list-clusters --region $AWS_REGION || echo "Failed to list clusters"
            exit 1
          fi
          
          # Update kubeconfig
          echo "üîÑ Updating kubeconfig..."
          aws eks update-kubeconfig --region $AWS_REGION --name $EKS_CLUSTER_NAME
          
          # Test cluster connectivity
          echo "üîó Testing cluster connectivity..."
          if kubectl cluster-info; then
            echo "‚úÖ Successfully connected to cluster"
          else
            echo "‚ùå Failed to connect to cluster"
            exit 1
          fi
          
          # Show current context
          echo "üìã Current kubectl context:"
          kubectl config current-context

      - name: "‚úÖ Verify Cluster Access"
        run: |
          echo "üîç Verifying cluster access and permissions..."
          
          # Check nodes
          echo "üñ•Ô∏è Cluster nodes:"
          kubectl get nodes -o wide
          
          # Check current namespace and permissions
          echo "üìç Current namespace:"
          kubectl config view --minify --output 'jsonpath={..namespace}' || echo 'default'
          
          # Test basic permissions
          echo "üîê Testing basic permissions:"
          kubectl auth can-i create namespaces
          kubectl auth can-i create deployments
          kubectl auth can-i create services
          kubectl auth can-i create configmaps
          kubectl auth can-i create secrets
          
          echo "‚úÖ Cluster access verification completed"

      # ---------------------------------------------------------
      # Setup Prerequisites (EBS CSI Driver, OIDC, IRSA)
      # ---------------------------------------------------------
      - name: "üîß Setup EBS CSI Driver with IRSA"
        if: github.event.inputs.action == 'install'
        run: |
          echo "üîß Setting up EBS CSI Driver prerequisites..."
          
          # Get cluster info
          CLUSTER_NAME="$EKS_CLUSTER_NAME"
          
          # Get OIDC issuer URL
          OIDC_ISSUER=$(aws eks describe-cluster --region $AWS_REGION --name $CLUSTER_NAME --query "cluster.identity.oidc.issuer" --output text)
          OIDC_ID=$(echo $OIDC_ISSUER | cut -d'/' -f5)
          
          echo "‚úÖ Cluster: $CLUSTER_NAME"
          echo "‚úÖ OIDC Issuer: $OIDC_ISSUER"
          echo "‚úÖ OIDC ID: $OIDC_ID"
          
          # Check if OIDC provider exists
          echo "üîç Checking OIDC provider..."
          OIDC_EXISTS=$(aws iam list-open-id-connect-providers --query "OpenIDConnectProviderList[?ends_with(Arn, '$OIDC_ID')].Arn" --output text)
          
          if [ -z "$OIDC_EXISTS" ]; then
            echo "üìù Creating OIDC provider..."
            aws iam create-open-id-connect-provider \
              --url $OIDC_ISSUER \
              --client-id-list sts.amazonaws.com \
              --thumbprint-list 9e99a48a9960b14926bb7f3b02e22da2b0ab7280
            echo "‚úÖ OIDC provider created"
          else
            echo "‚úÖ OIDC provider already exists: $OIDC_EXISTS"
          fi
          
          # Check if IAM role exists
          echo "üîç Checking EBS CSI IAM role..."
          ROLE_EXISTS=$(aws iam get-role --role-name AmazonEKS_EBS_CSI_DriverRole --query 'Role.RoleName' --output text 2>/dev/null || echo "")
          
          if [ "$ROLE_EXISTS" != "AmazonEKS_EBS_CSI_DriverRole" ]; then
            echo "üìù Creating EBS CSI IAM role..."
            
            # Create trust policy
            cat > ebs-csi-trust-policy.json << EOF
          {
            "Version": "2012-10-17", 
            "Statement": [
              {
                "Effect": "Allow",
                "Principal": {
                  "Federated": "arn:aws:iam::${{ vars.AWS_ACCOUNT_ID || secrets.AWS_ACCOUNT_ID }}:oidc-provider/${OIDC_ISSUER#https://}"
                },
                "Action": "sts:AssumeRoleWithWebIdentity",
                "Condition": {
                  "StringEquals": {
                    "${OIDC_ISSUER#https://}:sub": "system:serviceaccount:kube-system:ebs-csi-controller-sa",
                    "${OIDC_ISSUER#https://}:aud": "sts.amazonaws.com"
                  }
                }
              }
            ]
          }
          EOF
            
            # Create role and attach policy
            aws iam create-role --role-name AmazonEKS_EBS_CSI_DriverRole --assume-role-policy-document file://ebs-csi-trust-policy.json
            aws iam attach-role-policy --role-name AmazonEKS_EBS_CSI_DriverRole --policy-arn arn:aws:iam::aws:policy/service-role/AmazonEBSCSIDriverPolicy
            echo "‚úÖ EBS CSI IAM role created and policy attached"
          else
            echo "‚úÖ EBS CSI IAM role already exists"
          fi
          
          # Check if EBS CSI addon exists
          echo "üîç Checking EBS CSI addon..."
          ADDON_STATUS=$(aws eks describe-addon --cluster-name $CLUSTER_NAME --addon-name aws-ebs-csi-driver --region $AWS_REGION --query 'addon.status' --output text 2>/dev/null || echo "NOT_FOUND")
          
          if [ "$ADDON_STATUS" != "ACTIVE" ]; then
            echo "üìù Installing EBS CSI driver addon..."
            
            # Delete addon if it exists in failed state
            if [ "$ADDON_STATUS" != "NOT_FOUND" ]; then
              echo "üóëÔ∏è Removing existing addon in $ADDON_STATUS state..."
              aws eks delete-addon --cluster-name $CLUSTER_NAME --addon-name aws-ebs-csi-driver --region $AWS_REGION
              aws eks wait addon-deleted --cluster-name $CLUSTER_NAME --addon-name aws-ebs-csi-driver --region $AWS_REGION
            fi
            
            # Install addon with IAM role
            aws eks create-addon \
              --cluster-name $CLUSTER_NAME \
              --addon-name aws-ebs-csi-driver \
              --service-account-role-arn arn:aws:iam::${{ vars.AWS_ACCOUNT_ID || secrets.AWS_ACCOUNT_ID }}:role/AmazonEKS_EBS_CSI_DriverRole \
              --region $AWS_REGION
            
            echo "‚è≥ Waiting for EBS CSI driver to be active..."
            timeout 300 aws eks wait addon-active --cluster-name $CLUSTER_NAME --addon-name aws-ebs-csi-driver --region $AWS_REGION
            echo "‚úÖ EBS CSI driver installed and active"
          else
            echo "‚úÖ EBS CSI driver already active"
          fi
          
          # Verify EBS CSI driver pods are running
          echo "üîç Verifying EBS CSI driver pods..."
          kubectl get pods -n kube-system -l app=ebs-csi-controller
          echo "‚úÖ EBS CSI driver setup completed"

      # ---------------------------------------------------------
      # Install Monitoring Stack
      # ---------------------------------------------------------
      - name: "üì¶ Install Monitoring Stack"
        if: github.event.inputs.action == 'install'
        run: |
          echo "üìä Installing Prometheus and Grafana monitoring stack..."
          echo "üéØ Environment: ${{ github.event.inputs.environment }}"
          echo "‚è±Ô∏è Timeout: $HELM_TIMEOUT"
          
          # Make scripts executable
          chmod +x monitoring/install-monitoring.sh
          
          # Set environment variables for the script
          export ENVIRONMENT="${{ github.event.inputs.environment }}"
          export CLUSTER_NAME="$EKS_CLUSTER_NAME"
          
          # Run installation script
          cd monitoring
          ./install-monitoring.sh
          
          echo "‚úÖ Monitoring stack installation completed!"

      # ---------------------------------------------------------
      # ---------------------------------------------------------
      # PVC Status Check and Management
      # ---------------------------------------------------------
      - name: "üíæ PVC Status and Management"
        if: always()
        run: |
          echo "üíæ === PERSISTENT VOLUME CLAIMS STATUS ==="
          
          # Check if monitoring namespace exists
          if kubectl get namespace $MONITORING_NAMESPACE &> /dev/null; then
            echo "üìä Checking PVCs in $MONITORING_NAMESPACE namespace..."
            
            # List all PVCs with detailed information
            PVC_COUNT=$(kubectl get pvc -n $MONITORING_NAMESPACE --no-headers 2>/dev/null | wc -l || echo "0")
            
            if [ "$PVC_COUNT" -gt 0 ]; then
              echo "‚úÖ Found $PVC_COUNT Persistent Volume Claims:"
              echo
              kubectl get pvc -n $MONITORING_NAMESPACE -o wide
              echo
              
              # Show PVC storage usage if possible
              echo "üìà PVC Storage Details:"
              kubectl describe pvc -n $MONITORING_NAMESPACE | grep -E "Name:|Status:|Volume:|Capacity:|Access Modes:|Storage Class:"
              echo
              
              # Show related persistent volumes
              echo "üíΩ Related Persistent Volumes:"
              kubectl get pv | head -1  # Header
              kubectl get pvc -n $MONITORING_NAMESPACE --no-headers | awk '{print $3}' | while read pv; do
                if [ "$pv" != "<none>" ] && [ "$pv" != "" ]; then
                  kubectl get pv "$pv" --no-headers 2>/dev/null || true
                fi
              done
              echo
              
            else
              echo "‚ÑπÔ∏è  No PVCs found in $MONITORING_NAMESPACE namespace"
            fi
          else
            echo "‚ÑπÔ∏è  Monitoring namespace does not exist - no PVCs to check"
          fi
          
          # PVC cleanup reminder for uninstall operations
          if [ "${{ github.event.inputs.action }}" == "uninstall" ]; then
            echo "üóëÔ∏è  PVC CLEANUP INFORMATION:"
            echo "   ‚Ä¢ PVCs contain all monitoring data (metrics, dashboards, etc.)"
            echo "   ‚Ä¢ The uninstall script will prompt for PVC deletion"  
            echo "   ‚Ä¢ Choose 'y' to permanently delete all monitoring data"
            echo "   ‚Ä¢ Choose 'n' to keep PVCs for potential data recovery"
            echo
          fi

      # ---------------------------------------------------------
      # Uninstall Monitoring Stack  
      # ---------------------------------------------------------
      - name: "üóëÔ∏è Uninstall Monitoring Stack (with PVC Management)"
        if: github.event.inputs.action == 'uninstall'
        run: |
          echo "üóëÔ∏è Removing Prometheus and Grafana monitoring stack..."
          echo "üéØ Environment: ${{ github.event.inputs.environment }}"
          echo "üíæ PVC Management: Enhanced cleanup with data preservation options"
          
          # Make scripts executable
          chmod +x monitoring/uninstall-monitoring.sh
          
          # Set environment variables for the script
          export ENVIRONMENT="${{ github.event.inputs.environment }}"
          export CLUSTER_NAME="$EKS_CLUSTER_NAME"
          
          echo "üìã Pre-cleanup PVC Status:"
          kubectl get pvc -n $MONITORING_NAMESPACE || echo "No PVCs found"
          echo
          
          # Run uninstallation script with enhanced PVC handling
          cd monitoring
          echo "ü§ñ Running automated cleanup with PVC confirmation..."
          
          # Auto-confirm main uninstall but let user decide on PVCs in interactive mode
          # For CI/CD, we'll delete PVCs automatically, but show what's happening
          echo "y" | ./uninstall-monitoring.sh
          
          echo "üìã Post-cleanup PVC Status:"
          kubectl get pvc -n $MONITORING_NAMESPACE 2>/dev/null || echo "‚úÖ No PVCs remaining (deleted or namespace removed)"
          
          echo "‚úÖ Monitoring stack removal completed with PVC management!"

      # ---------------------------------------------------------
      # Check Monitoring Status
      # ---------------------------------------------------------
      - name: "üìã Check Monitoring Status"
        if: always()
        run: |
          echo "üìä === MONITORING STACK STATUS ==="
          echo "üîñ Environment: ${{ github.event.inputs.environment || 'dev' }}"
          echo "üéØ EKS Cluster: $EKS_CLUSTER_NAME"
          echo "üåç AWS Region: $AWS_REGION"
          echo "‚ö° Action Performed: ${{ github.event.inputs.action }}"
          echo
          
          # Check if monitoring namespace exists
          if kubectl get namespace $MONITORING_NAMESPACE &> /dev/null; then
            echo "‚úÖ Monitoring namespace exists"
            echo
            
            # Check Helm releases in monitoring namespace
            echo "üé° Helm Releases in monitoring namespace:"
            helm list -n $MONITORING_NAMESPACE || echo "No Helm releases found"
            echo
            
            # Check pods in monitoring namespace
            echo "üê≥ Pods in monitoring namespace:"
            kubectl get pods -n $MONITORING_NAMESPACE -o wide || echo "No pods found"
            echo
            
            # Check services in monitoring namespace
            echo "üåê Services in monitoring namespace:"
            kubectl get services -n $MONITORING_NAMESPACE || echo "No services found"
            echo
            
            # Check persistent volumes
            echo "üíæ Persistent Volume Claims:"
            kubectl get pvc -n $MONITORING_NAMESPACE || echo "No PVCs found"
            echo
            
            # If Grafana is running, show access information
            if kubectl get pods -n $MONITORING_NAMESPACE -l "app.kubernetes.io/name=grafana" | grep -q "Running"; then
              echo "üìä Grafana Access Information:"
              echo "   1. Port forward: kubectl port-forward -n $MONITORING_NAMESPACE svc/kube-prometheus-stack-grafana 3000:80"
              echo "   2. URL: http://localhost:3000"
              echo "   3. Username: admin"
              echo "   4. Get password: kubectl get secret -n $MONITORING_NAMESPACE kube-prometheus-stack-grafana -o jsonpath=\"{.data.admin-password}\" | base64 --decode && echo"
              echo
            fi
            
            # If Prometheus is running, show access information
            if kubectl get pods -n $MONITORING_NAMESPACE -l "app.kubernetes.io/name=prometheus" | grep -q "Running"; then
              echo "üîç Prometheus Access Information:"
              echo "   1. Port forward: kubectl port-forward -n $MONITORING_NAMESPACE svc/kube-prometheus-stack-prometheus 9090:9090"
              echo "   2. URL: http://localhost:9090"
              echo "   3. Check targets: http://localhost:9090/targets"
              echo
            fi
          else
            echo "‚ùå Monitoring namespace not found"
            echo "üí° Run this workflow with action 'install' to deploy monitoring stack"
          fi

      # ---------------------------------------------------------
      # Generate Monitoring Report
      # ---------------------------------------------------------
      - name: "üìä Generate Monitoring Report"
        if: always()
        run: |
          echo "# üìä Monitoring Stack Deployment Report" > monitoring-report.md
          echo "" >> monitoring-report.md
          echo "**Date:** $(date -u)" >> monitoring-report.md
          echo "**Environment:** ${{ github.event.inputs.environment || 'dev' }}" >> monitoring-report.md
          echo "**Action:** ${{ github.event.inputs.action }}" >> monitoring-report.md
          echo "**EKS Cluster:** $EKS_CLUSTER_NAME" >> monitoring-report.md
          echo "**AWS Region:** $AWS_REGION" >> monitoring-report.md
          echo "**Workflow Run:** ${{ github.run_id }}" >> monitoring-report.md
          echo "" >> monitoring-report.md
          
          # Add status information
          if kubectl get namespace $MONITORING_NAMESPACE &> /dev/null; then
            echo "## ‚úÖ Monitoring Stack Status: ACTIVE" >> monitoring-report.md
            echo "" >> monitoring-report.md
            echo "### Components:" >> monitoring-report.md
            echo '```' >> monitoring-report.md
            kubectl get pods -n $MONITORING_NAMESPACE >> monitoring-report.md 2>/dev/null || echo "No pods found" >> monitoring-report.md
            echo '```' >> monitoring-report.md
            echo "" >> monitoring-report.md
            echo "### Services:" >> monitoring-report.md
            echo '```' >> monitoring-report.md
            kubectl get svc -n $MONITORING_NAMESPACE >> monitoring-report.md 2>/dev/null || echo "No services found" >> monitoring-report.md
            echo '```' >> monitoring-report.md
          else
            echo "## ‚ùå Monitoring Stack Status: NOT INSTALLED" >> monitoring-report.md
          fi
          
          echo "" >> monitoring-report.md
          echo "### Quick Access Commands:" >> monitoring-report.md
          echo "" >> monitoring-report.md
          echo "**Grafana:**" >> monitoring-report.md
          echo '```bash' >> monitoring-report.md
          echo "kubectl port-forward -n $MONITORING_NAMESPACE svc/kube-prometheus-stack-grafana 3000:80" >> monitoring-report.md
          echo '```' >> monitoring-report.md
          echo "" >> monitoring-report.md
          echo "**Prometheus:**" >> monitoring-report.md
          echo '```bash' >> monitoring-report.md
          echo "kubectl port-forward -n $MONITORING_NAMESPACE svc/kube-prometheus-stack-prometheus 9090:9090" >> monitoring-report.md
          echo '```' >> monitoring-report.md
          echo "" >> monitoring-report.md
          echo "**Get Grafana Password:**" >> monitoring-report.md
          echo '```bash' >> monitoring-report.md
          echo "kubectl get secret -n $MONITORING_NAMESPACE kube-prometheus-stack-grafana -o jsonpath=\"{.data.admin-password}\" | base64 --decode && echo" >> monitoring-report.md
          echo '```' >> monitoring-report.md
          
          echo "üìù Monitoring report generated: monitoring-report.md"

      # ---------------------------------------------------------
      # Upload Report Artifact
      # ---------------------------------------------------------
      - name: "üì§ Upload Monitoring Report"
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: monitoring-report-${{ github.event.inputs.environment }}-${{ github.run_id }}
          path: monitoring-report.md
          retention-days: 30

      # ---------------------------------------------------------
      # Final Status Notification
      # ---------------------------------------------------------
      - name: "üéâ Installation Success Notification"
        if: success() && github.event.inputs.action == 'install'
        run: |
          echo "‚úÖ Monitoring Stack Successfully Installed!"
          echo "üéØ Environment: ${{ github.event.inputs.environment }}"
          echo "üè∑Ô∏è EKS Cluster: $EKS_CLUSTER_NAME"
          echo ""
          echo "ÔøΩ Storage Information:"
          PVC_COUNT=$(kubectl get pvc -n $MONITORING_NAMESPACE --no-headers 2>/dev/null | wc -l || echo "0")
          echo "üìä Created $PVC_COUNT Persistent Volume Claims for data persistence"
          kubectl get pvc -n $MONITORING_NAMESPACE -o custom-columns=NAME:.metadata.name,STATUS:.status.phase,CAPACITY:.status.capacity.storage 2>/dev/null || true
          echo ""
          echo "ÔøΩüìä Next Steps:"
          echo "1. Access Grafana: kubectl port-forward -n $MONITORING_NAMESPACE svc/kube-prometheus-stack-grafana 3000:80"
          echo "2. Open http://localhost:3000"
          echo "3. Login with admin/[get password from secret]"
          echo "4. Explore pre-built Kubernetes dashboards"
          echo ""
          echo "üîç Prometheus: kubectl port-forward -n $MONITORING_NAMESPACE svc/kube-prometheus-stack-prometheus 9090:9090"

      - name: "‚úÖ Uninstallation Success Notification"
        if: success() && github.event.inputs.action == 'uninstall'
        run: |
          echo "‚úÖ Monitoring Stack Successfully Removed!"
          echo "üéØ Environment: ${{ github.event.inputs.environment }}"
          echo "üè∑Ô∏è EKS Cluster: $EKS_CLUSTER_NAME"
          echo ""
          echo "ÔøΩ PVC Cleanup Status:"
          PVC_COUNT=$(kubectl get pvc -n $MONITORING_NAMESPACE --no-headers 2>/dev/null | wc -l || echo "0")
          if [ "$PVC_COUNT" -eq 0 ]; then
            echo "‚úÖ All PVCs removed - monitoring data permanently deleted"
          else
            echo "‚ö†Ô∏è  $PVC_COUNT PVCs preserved - monitoring data retained"
            kubectl get pvc -n $MONITORING_NAMESPACE
            echo "üí° To remove data: kubectl delete pvc --all -n $MONITORING_NAMESPACE"
          fi
          echo ""
          echo "üóëÔ∏è Removed Components:"
          echo "- Prometheus Server"
          echo "- Grafana Dashboards"
          echo "- AlertManager"
          echo "- Node Exporter"
          echo "- kube-state-metrics"
          echo "- Prometheus Operator"
          echo "- Custom Resource Definitions (CRDs)"
          echo ""
          echo "üí° To reinstall: Run this workflow again with action 'install'"

      - name: "üö® Failure Notification"
        if: failure()
        run: |
          echo "‚ùå Monitoring Stack Operation Failed!"
          echo "üéØ Environment: ${{ github.event.inputs.environment }}"
          echo "‚ö° Action: ${{ github.event.inputs.action }}"
          echo "üè∑Ô∏è EKS Cluster: $EKS_CLUSTER_NAME"
          echo ""
          echo "üîç Troubleshooting Steps:"
          echo "1. Check the logs above for specific error messages"
          echo "2. Verify EKS cluster is accessible"
          echo "3. Check AWS credentials and permissions"
          echo "4. Verify Helm is properly installed"
          echo "5. Check if resources already exist (for install) or don't exist (for uninstall)"
          echo ""
          echo "üìã Get current status:"
          echo "kubectl get all -n $MONITORING_NAMESPACE"
          echo "helm list -n $MONITORING_NAMESPACE"